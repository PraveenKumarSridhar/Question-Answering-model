{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbef6039",
   "metadata": {},
   "source": [
    "# Question Answering model - Information Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6dc7d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f1a307",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import swifter\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
    "import gensim.downloader\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c28db3",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312579fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "2  56be85543aeaaa14008c9066  Beyoncé   \n",
       "3  56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4  56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "   answer_start  is_impossible  \n",
       "0           269          False  \n",
       "1           207          False  \n",
       "2           526          False  \n",
       "3           166          False  \n",
       "4           276          False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir =  os.path.dirname(os.getcwd())\n",
    "train_df = pd.read_csv(os.path.join(path_dir,r'data\\interim\\train_data.csv'))\n",
    "val_df = pd.read_csv(os.path.join(path_dir,r'data\\interim\\val_data.csv'))\n",
    "train_df.drop('Unnamed: 0',axis=1,inplace = True)\n",
    "val_df.drop('Unnamed: 0',axis=1,inplace = True)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a148b",
   "metadata": {},
   "source": [
    "### Get whole answer sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56401f65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_answer_context(df):\n",
    "    length_context = 0\n",
    "    answer = \"\"\n",
    "\n",
    "    for sentence in sent_tokenize(df.context):\n",
    "        length_context += len(sentence) + 1\n",
    "        if df.answer_start <= length_context:\n",
    "            if len(sentence) >= len(str(df.answer)):\n",
    "                if answer == \"\":\n",
    "                    return sentence\n",
    "                else:\n",
    "                    return answer + \" \" + sentence\n",
    "            else:\n",
    "                answer += sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a75783",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 86820/86820 [00:13<00:00, 6624.22it/s]\n",
      "100%|███████████████████████████| 20302/20302 [00:02<00:00, 8102.40it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['answer_sentences'] = train_df.progress_apply(lambda row: get_answer_context(row),axis = 1)\n",
    "val_df['answer_sentences'] = val_df.progress_apply(lambda row: get_answer_context(row),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e537e30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>False</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "2  56be85543aeaaa14008c9066  Beyoncé   \n",
       "3  56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4  56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "   answer_start  is_impossible  \\\n",
       "0           269          False   \n",
       "1           207          False   \n",
       "2           526          False   \n",
       "3           166          False   \n",
       "4           276          False   \n",
       "\n",
       "                                    answer_sentences  \n",
       "0  Born and raised in Houston, Texas, she perform...  \n",
       "1  Born and raised in Houston, Texas, she perform...  \n",
       "2  Their hiatus saw the release of Beyoncé's debu...  \n",
       "3  Born and raised in Houston, Texas, she perform...  \n",
       "4  Born and raised in Houston, Texas, she perform...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e89511",
   "metadata": {},
   "source": [
    "### Preprocess context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "335ee761",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 18877/18877 [00:01<00:00, 10953.26it/s]\n",
      "100%|██████████████████████████| 86768/86768 [00:00<00:00, 92444.06it/s]\n"
     ]
    }
   ],
   "source": [
    "context_df = pd.DataFrame(train_df['context'].unique().tolist(),columns=['context'])\n",
    "context_df['processed'] = context_df['context'].progress_apply(lambda x: simple_preprocess(x))\n",
    "\n",
    "question_df = pd.DataFrame(train_df['question'].unique().tolist(),columns=['question'])\n",
    "question_df['processed'] = question_df['question'].progress_apply(lambda x: simple_preprocess(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1f63f",
   "metadata": {},
   "source": [
    "### Training a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c027f27f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 105645/105645 [17:10<00:00, 102.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sentences = context_df['processed'].tolist() + question_df['processed'].tolist()\n",
    "train_words = list(itertools.chain(*train_sentences))\n",
    "low_word_count = 1\n",
    "word_count_dict = Counter(train_words)\n",
    "low_freq_words = [k for k, v in word_count_dict.items() if v == low_word_count]\n",
    "UNK = '<UNK>'\n",
    "processed_train_sentences = [[word if word not in low_freq_words else UNK for word in sentence]\n",
    "                            for sentence in tqdm(train_sentences)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa019eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# init callback class\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "3947f6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 10414\n",
      "Loss after epoch 0: 508614.78125\n",
      "Loss after epoch 1: 447039.09375\n",
      "Loss after epoch 2: 436167.125\n",
      "Loss after epoch 3: 402817.375\n",
      "Loss after epoch 4: 386353.125\n",
      "Loss after epoch 5: 366140.5\n",
      "Loss after epoch 6: 357714.75\n",
      "Loss after epoch 7: 352439.5\n",
      "Loss after epoch 8: 345756.75\n",
      "Loss after epoch 9: 355441.75\n",
      "Loss after epoch 10: 330795.25\n",
      "Loss after epoch 11: 305028.5\n",
      "Loss after epoch 12: 322829.0\n",
      "Loss after epoch 13: 308551.0\n",
      "Loss after epoch 14: 317928.5\n",
      "Loss after epoch 15: 307165.0\n",
      "Loss after epoch 16: 305995.0\n",
      "Loss after epoch 17: 315734.0\n",
      "Loss after epoch 18: 302671.5\n",
      "Loss after epoch 19: 305039.0\n",
      "Loss after epoch 20: 301068.0\n",
      "Loss after epoch 21: 305026.5\n",
      "Loss after epoch 22: 304666.5\n",
      "Loss after epoch 23: 301880.5\n",
      "Loss after epoch 24: 280917.0\n",
      "Loss after epoch 25: 287391.0\n",
      "Loss after epoch 26: 278628.0\n",
      "Loss after epoch 27: 278125.0\n",
      "Loss after epoch 28: 288754.0\n",
      "Loss after epoch 29: 281670.0\n",
      "Loss after epoch 30: 299383.0\n",
      "Loss after epoch 31: 279910.0\n",
      "Loss after epoch 32: 290374.0\n",
      "Loss after epoch 33: 283705.0\n",
      "Loss after epoch 34: 270884.0\n",
      "Loss after epoch 35: 281929.0\n",
      "Loss after epoch 36: 288160.0\n",
      "Loss after epoch 37: 290063.0\n",
      "Loss after epoch 38: 282133.0\n",
      "Loss after epoch 39: 278041.0\n",
      "Loss after epoch 40: 284709.0\n",
      "Loss after epoch 41: 301139.0\n",
      "Loss after epoch 42: 293551.0\n",
      "Loss after epoch 43: 284317.0\n",
      "Loss after epoch 44: 282892.0\n",
      "Loss after epoch 45: 295695.0\n",
      "Loss after epoch 46: 298875.0\n",
      "Loss after epoch 47: 293528.0\n",
      "Loss after epoch 48: 292565.0\n",
      "Loss after epoch 49: 277159.0\n",
      "Loss after epoch 50: 282472.0\n",
      "Loss after epoch 51: 273829.0\n",
      "Loss after epoch 52: 291685.0\n",
      "Loss after epoch 53: 292314.0\n",
      "Loss after epoch 54: 258460.0\n",
      "Loss after epoch 55: 247004.0\n",
      "Loss after epoch 56: 247170.0\n",
      "Loss after epoch 57: 258170.0\n",
      "Loss after epoch 58: 256574.0\n",
      "Loss after epoch 59: 257968.0\n",
      "Loss after epoch 60: 256372.0\n",
      "Loss after epoch 61: 250210.0\n",
      "Loss after epoch 62: 248478.0\n",
      "Loss after epoch 63: 261038.0\n",
      "Loss after epoch 64: 248222.0\n",
      "Loss after epoch 65: 251856.0\n",
      "Loss after epoch 66: 241234.0\n",
      "Loss after epoch 67: 251114.0\n",
      "Loss after epoch 68: 244652.0\n",
      "Loss after epoch 69: 250982.0\n",
      "Loss after epoch 70: 244050.0\n",
      "Loss after epoch 71: 248546.0\n",
      "Loss after epoch 72: 247206.0\n",
      "Loss after epoch 73: 250758.0\n",
      "Loss after epoch 74: 251844.0\n",
      "Loss after epoch 75: 250570.0\n",
      "Loss after epoch 76: 252292.0\n",
      "Loss after epoch 77: 254608.0\n",
      "Loss after epoch 78: 248206.0\n",
      "Loss after epoch 79: 247544.0\n",
      "Loss after epoch 80: 250674.0\n",
      "Loss after epoch 81: 276400.0\n",
      "Loss after epoch 82: 250626.0\n",
      "Loss after epoch 83: 242282.0\n",
      "Loss after epoch 84: 259588.0\n",
      "Loss after epoch 85: 258218.0\n",
      "Loss after epoch 86: 259966.0\n",
      "Loss after epoch 87: 248210.0\n",
      "Loss after epoch 88: 256746.0\n",
      "Loss after epoch 89: 256254.0\n",
      "Loss after epoch 90: 248138.0\n",
      "Loss after epoch 91: 253248.0\n",
      "Loss after epoch 92: 251428.0\n",
      "Loss after epoch 93: 251864.0\n",
      "Loss after epoch 94: 237898.0\n",
      "Loss after epoch 95: 257680.0\n",
      "Loss after epoch 96: 253296.0\n",
      "Loss after epoch 97: 249552.0\n",
      "Loss after epoch 98: 254164.0\n",
      "Loss after epoch 99: 257250.0\n",
      "Loss after epoch 100: 240960.0\n",
      "Loss after epoch 101: 258946.0\n",
      "Loss after epoch 102: 248850.0\n",
      "Loss after epoch 103: 255912.0\n",
      "Loss after epoch 104: 238406.0\n",
      "Loss after epoch 105: 249854.0\n",
      "Loss after epoch 106: 257942.0\n",
      "Loss after epoch 107: 248728.0\n",
      "Loss after epoch 108: 251798.0\n",
      "Loss after epoch 109: 259342.0\n",
      "Loss after epoch 110: 255108.0\n",
      "Loss after epoch 111: 256032.0\n",
      "Loss after epoch 112: 254932.0\n",
      "Loss after epoch 113: 266412.0\n",
      "Loss after epoch 114: 249172.0\n",
      "Loss after epoch 115: 255096.0\n",
      "Loss after epoch 116: 247270.0\n",
      "Loss after epoch 117: 246736.0\n",
      "Loss after epoch 118: 247754.0\n",
      "Loss after epoch 119: 271072.0\n",
      "Loss after epoch 120: 198272.0\n",
      "Loss after epoch 121: 197808.0\n",
      "Loss after epoch 122: 184352.0\n",
      "Loss after epoch 123: 191636.0\n",
      "Loss after epoch 124: 179652.0\n",
      "Loss after epoch 125: 192400.0\n",
      "Loss after epoch 126: 194348.0\n",
      "Loss after epoch 127: 180624.0\n",
      "Loss after epoch 128: 185596.0\n",
      "Loss after epoch 129: 198536.0\n",
      "Loss after epoch 130: 183300.0\n",
      "Loss after epoch 131: 183632.0\n",
      "Loss after epoch 132: 193028.0\n",
      "Loss after epoch 133: 178332.0\n",
      "Loss after epoch 134: 183644.0\n",
      "Loss after epoch 135: 184696.0\n",
      "Loss after epoch 136: 189892.0\n",
      "Loss after epoch 137: 190988.0\n",
      "Loss after epoch 138: 182232.0\n",
      "Loss after epoch 139: 183164.0\n",
      "Loss after epoch 140: 190016.0\n",
      "Loss after epoch 141: 183548.0\n",
      "Loss after epoch 142: 194596.0\n",
      "Loss after epoch 143: 182744.0\n",
      "Loss after epoch 144: 182912.0\n",
      "Loss after epoch 145: 181100.0\n",
      "Loss after epoch 146: 182384.0\n",
      "Loss after epoch 147: 181408.0\n",
      "Loss after epoch 148: 183460.0\n",
      "Loss after epoch 149: 174956.0\n",
      "Loss after epoch 150: 174916.0\n",
      "Loss after epoch 151: 186304.0\n",
      "Loss after epoch 152: 180888.0\n",
      "Loss after epoch 153: 181856.0\n",
      "Loss after epoch 154: 172708.0\n",
      "Loss after epoch 155: 183592.0\n",
      "Loss after epoch 156: 178568.0\n",
      "Loss after epoch 157: 180984.0\n",
      "Loss after epoch 158: 185316.0\n",
      "Loss after epoch 159: 178644.0\n",
      "Loss after epoch 160: 186244.0\n",
      "Loss after epoch 161: 176468.0\n",
      "Loss after epoch 162: 181328.0\n",
      "Loss after epoch 163: 169704.0\n",
      "Loss after epoch 164: 174116.0\n",
      "Loss after epoch 165: 169988.0\n",
      "Loss after epoch 166: 176800.0\n",
      "Loss after epoch 167: 176020.0\n",
      "Loss after epoch 168: 168828.0\n",
      "Loss after epoch 169: 181436.0\n",
      "Loss after epoch 170: 176832.0\n",
      "Loss after epoch 171: 176552.0\n",
      "Loss after epoch 172: 182120.0\n",
      "Loss after epoch 173: 170924.0\n",
      "Loss after epoch 174: 188592.0\n",
      "Loss after epoch 175: 184328.0\n",
      "Loss after epoch 176: 164716.0\n",
      "Loss after epoch 177: 173732.0\n",
      "Loss after epoch 178: 176528.0\n",
      "Loss after epoch 179: 166712.0\n",
      "Loss after epoch 180: 169380.0\n",
      "Loss after epoch 181: 172636.0\n",
      "Loss after epoch 182: 170348.0\n",
      "Loss after epoch 183: 172756.0\n",
      "Loss after epoch 184: 183320.0\n",
      "Loss after epoch 185: 162032.0\n",
      "Loss after epoch 186: 168668.0\n",
      "Loss after epoch 187: 170224.0\n",
      "Loss after epoch 188: 177132.0\n",
      "Loss after epoch 189: 164744.0\n",
      "Loss after epoch 190: 174312.0\n",
      "Loss after epoch 191: 170652.0\n",
      "Loss after epoch 192: 166620.0\n",
      "Loss after epoch 193: 169912.0\n",
      "Loss after epoch 194: 162776.0\n",
      "Loss after epoch 195: 174920.0\n",
      "Loss after epoch 196: 175220.0\n",
      "Loss after epoch 197: 161336.0\n",
      "Loss after epoch 198: 161628.0\n",
      "Loss after epoch 199: 165960.0\n",
      "Loss after epoch 200: 173624.0\n",
      "Loss after epoch 201: 170624.0\n",
      "Loss after epoch 202: 161384.0\n",
      "Loss after epoch 203: 167608.0\n",
      "Loss after epoch 204: 164944.0\n",
      "Loss after epoch 205: 169252.0\n",
      "Loss after epoch 206: 164144.0\n",
      "Loss after epoch 207: 158152.0\n",
      "Loss after epoch 208: 164168.0\n",
      "Loss after epoch 209: 169608.0\n",
      "Loss after epoch 210: 161080.0\n",
      "Loss after epoch 211: 170532.0\n",
      "Loss after epoch 212: 160268.0\n",
      "Loss after epoch 213: 166936.0\n",
      "Loss after epoch 214: 167888.0\n",
      "Loss after epoch 215: 160424.0\n",
      "Loss after epoch 216: 160276.0\n",
      "Loss after epoch 217: 176408.0\n",
      "Loss after epoch 218: 158432.0\n",
      "Loss after epoch 219: 164272.0\n",
      "Loss after epoch 220: 162856.0\n",
      "Loss after epoch 221: 157460.0\n",
      "Loss after epoch 222: 162920.0\n",
      "Loss after epoch 223: 158840.0\n",
      "Loss after epoch 224: 158336.0\n",
      "Loss after epoch 225: 163880.0\n",
      "Loss after epoch 226: 161164.0\n",
      "Loss after epoch 227: 156784.0\n",
      "Loss after epoch 228: 159712.0\n",
      "Loss after epoch 229: 147400.0\n",
      "Loss after epoch 230: 159080.0\n",
      "Loss after epoch 231: 165032.0\n",
      "Loss after epoch 232: 166392.0\n",
      "Loss after epoch 233: 154664.0\n",
      "Loss after epoch 234: 159264.0\n",
      "Loss after epoch 235: 152336.0\n",
      "Loss after epoch 236: 151284.0\n",
      "Loss after epoch 237: 162600.0\n",
      "Loss after epoch 238: 152136.0\n",
      "Loss after epoch 239: 153032.0\n",
      "Loss after epoch 240: 149056.0\n",
      "Loss after epoch 241: 143620.0\n",
      "Loss after epoch 242: 145240.0\n",
      "Loss after epoch 243: 148780.0\n",
      "Loss after epoch 244: 149096.0\n",
      "Loss after epoch 245: 151032.0\n",
      "Loss after epoch 246: 142788.0\n",
      "Loss after epoch 247: 144536.0\n",
      "Loss after epoch 248: 148012.0\n",
      "Loss after epoch 249: 150300.0\n",
      "Loss after epoch 250: 143516.0\n",
      "Loss after epoch 251: 144732.0\n",
      "Loss after epoch 252: 144444.0\n",
      "Loss after epoch 253: 139264.0\n",
      "Loss after epoch 254: 143780.0\n",
      "Loss after epoch 255: 143960.0\n",
      "Loss after epoch 256: 141464.0\n",
      "Loss after epoch 257: 146632.0\n",
      "Loss after epoch 258: 138928.0\n",
      "Loss after epoch 259: 143700.0\n",
      "Loss after epoch 260: 145012.0\n",
      "Loss after epoch 261: 139768.0\n",
      "Loss after epoch 262: 143644.0\n",
      "Loss after epoch 263: 141672.0\n",
      "Loss after epoch 264: 137804.0\n",
      "Loss after epoch 265: 140976.0\n",
      "Loss after epoch 266: 141932.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 267: 135284.0\n",
      "Loss after epoch 268: 135116.0\n",
      "Loss after epoch 269: 139432.0\n",
      "Loss after epoch 270: 139712.0\n",
      "Loss after epoch 271: 137572.0\n",
      "Loss after epoch 272: 128348.0\n",
      "Loss after epoch 273: 131448.0\n",
      "Loss after epoch 274: 134184.0\n",
      "Loss after epoch 275: 131588.0\n",
      "Loss after epoch 276: 137476.0\n",
      "Loss after epoch 277: 133940.0\n",
      "Loss after epoch 278: 131924.0\n",
      "Loss after epoch 279: 124240.0\n",
      "Loss after epoch 280: 129612.0\n",
      "Loss after epoch 281: 127428.0\n",
      "Loss after epoch 282: 122960.0\n",
      "Loss after epoch 283: 125820.0\n",
      "Loss after epoch 284: 126860.0\n",
      "Loss after epoch 285: 128540.0\n",
      "Loss after epoch 286: 132484.0\n",
      "Loss after epoch 287: 118700.0\n",
      "Loss after epoch 288: 117960.0\n",
      "Loss after epoch 289: 126368.0\n",
      "Loss after epoch 290: 123104.0\n",
      "Loss after epoch 291: 115880.0\n",
      "Loss after epoch 292: 114596.0\n",
      "Loss after epoch 293: 115772.0\n",
      "Loss after epoch 294: 118188.0\n",
      "Loss after epoch 295: 119352.0\n",
      "Loss after epoch 296: 113560.0\n",
      "Loss after epoch 297: 111892.0\n",
      "Loss after epoch 298: 111172.0\n",
      "Loss after epoch 299: 117780.0\n",
      "Loss after epoch 300: 112756.0\n",
      "Loss after epoch 301: 112772.0\n",
      "Loss after epoch 302: 110460.0\n",
      "Loss after epoch 303: 105328.0\n",
      "Loss after epoch 304: 108572.0\n",
      "Loss after epoch 305: 108912.0\n",
      "Loss after epoch 306: 114116.0\n",
      "Loss after epoch 307: 106276.0\n",
      "Loss after epoch 308: 110768.0\n",
      "Loss after epoch 309: 101848.0\n",
      "Loss after epoch 310: 105780.0\n",
      "Loss after epoch 311: 105152.0\n",
      "Loss after epoch 312: 99660.0\n",
      "Loss after epoch 313: 103752.0\n",
      "Loss after epoch 314: 102460.0\n",
      "Loss after epoch 315: 104632.0\n",
      "Loss after epoch 316: 102504.0\n",
      "Loss after epoch 317: 102120.0\n",
      "Loss after epoch 318: 102596.0\n",
      "Loss after epoch 319: 97932.0\n",
      "Loss after epoch 320: 99176.0\n",
      "Loss after epoch 321: 99524.0\n",
      "Loss after epoch 322: 102632.0\n",
      "Loss after epoch 323: 90464.0\n",
      "Loss after epoch 324: 90464.0\n",
      "Loss after epoch 325: 88092.0\n",
      "Loss after epoch 326: 92372.0\n",
      "Loss after epoch 327: 88172.0\n",
      "Loss after epoch 328: 91072.0\n",
      "Loss after epoch 329: 93760.0\n",
      "Loss after epoch 330: 90396.0\n",
      "Loss after epoch 331: 87596.0\n",
      "Loss after epoch 332: 92620.0\n",
      "Loss after epoch 333: 87612.0\n",
      "Loss after epoch 334: 88616.0\n",
      "Loss after epoch 335: 86048.0\n",
      "Loss after epoch 336: 82160.0\n",
      "Loss after epoch 337: 83284.0\n",
      "Loss after epoch 338: 86144.0\n",
      "Loss after epoch 339: 79412.0\n",
      "Loss after epoch 340: 81640.0\n",
      "Loss after epoch 341: 80636.0\n",
      "Loss after epoch 342: 79808.0\n",
      "Loss after epoch 343: 80760.0\n",
      "Loss after epoch 344: 76832.0\n",
      "Loss after epoch 345: 78864.0\n",
      "Loss after epoch 346: 80620.0\n",
      "Loss after epoch 347: 79744.0\n",
      "Loss after epoch 348: 78248.0\n",
      "Loss after epoch 349: 79048.0\n",
      "67091556.0\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300\n",
    "w2v_model = Word2Vec(min_count=20, \n",
    "                     window = 20,\n",
    "                     vector_size = vector_size,\n",
    "                     workers=10)\n",
    "\n",
    "w2v_model.build_vocab(processed_train_sentences)\n",
    "words = w2v_model.wv.key_to_index.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)\n",
    "# Train Word Embeddings\n",
    "w2v_model.train(processed_train_sentences, \n",
    "                total_examples=w2v_model.corpus_count, \n",
    "                epochs=350, \n",
    "                report_delay=1,\n",
    "                compute_loss = True, # set compute_loss = True\n",
    "                callbacks=[callback()]) \n",
    "print(w2v_model.get_latest_training_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "47c9fb16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('least', 0.40335699915885925),\n",
       " ('expense', 0.2993525564670563),\n",
       " ('age', 0.29746243357658386),\n",
       " ('level', 0.274740070104599),\n",
       " ('point', 0.2606557607650757),\n",
       " ('rate', 0.2550274431705475),\n",
       " ('times', 0.2353343516588211),\n",
       " ('night', 0.23298636078834534),\n",
       " ('beginning', 0.22899189591407776),\n",
       " ('end', 0.22744593024253845)]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "d661cb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'their' in w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "1e731572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model, num_features):\n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        word_vec_model = model.wv\n",
    "    else:\n",
    "        word_vec_model = model\n",
    "    index2word_set = word_vec_model.index_to_key \n",
    "    #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, word_vec_model[word])\n",
    "\n",
    "    if nwords>0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "#     print(featureVec)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "d4be362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_sentence_vector(train_df['question'].tolist()[0].split(),w2v_model,100) \n",
    "# avg_sentence_vector(train_df['question'].tolist()[1].split(),w2v_model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "f9ce5bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity(context,question,model,vector_size=300):\n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        vocab = model.wv.key_to_index\n",
    "    else:\n",
    "        vocab = model.key_to_index\n",
    "#     print(context,question)\n",
    "    context_sents = sent_tokenize(context)\n",
    "#     print(context_sents)\n",
    "    processed_context = [simple_preprocess(sent) for sent in context_sents]\n",
    "    processed_context = [[word if word in vocab else UNK for word in processed_context_sent]\\\n",
    "                         for processed_context_sent in processed_context]\n",
    "#     print(processed_context)\n",
    "    processed_question = simple_preprocess(question)\n",
    "    processed_question = [word if word in vocab else UNK for word in processed_question]\n",
    "    \n",
    "    context_vectors = [np.array(avg_sentence_vector(processed_context_sent,model,vector_size)).reshape(1,-1) for processed_context_sent in processed_context]\n",
    "    question_vector  = np.array(avg_sentence_vector(processed_question,model,vector_size)).reshape(1,-1)\n",
    "#     print(len(context_vectors[0]))\n",
    "#     print(cosine_similarity(np.array(context_vectors[0]).reshape(1,-1),np.array(question_vector).reshape(1,-1)))\n",
    "    \n",
    "    cosine_sim_list = [cosine_similarity(context_sent_vector,question_vector) for context_sent_vector in context_vectors]\n",
    "    \n",
    "#     print(f\"Cosine scores: {cosine_sim_list}\")\n",
    "    max_cosine_sim = max(cosine_sim_list)\n",
    "    predicted_answer = context_sents[np.argmax(cosine_sim_list)]\n",
    "    return max_cosine_sim, predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "0932bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Q: What areas did Beyonce compete in when she was growing up?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.21846901]], dtype=float32),\n",
       " \"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\")"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_context = train_df['context'].tolist()[0]\n",
    "sample_question = train_df['question'].tolist()[1]\n",
    "print(f\"C:{sample_context}\")\n",
    "print(f\"Q: {sample_question}\")\n",
    "get_cosine_similarity(sample_context,sample_question,w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "dece55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c2fa186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 218.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine scores: [array([[0.04423445]], dtype=float32), array([[0.16496505]], dtype=float32), array([[0.0983622]], dtype=float32), array([[0.22614476]], dtype=float32)]\n",
      "Cosine scores: [array([[0.09085849]], dtype=float32), array([[0.21846901]], dtype=float32), array([[0.12052556]], dtype=float32), array([[0.20577396]], dtype=float32)]\n",
      "Cosine scores: [array([[0.33140668]], dtype=float32), array([[0.50080836]], dtype=float32), array([[0.28680265]], dtype=float32), array([[0.44583791]], dtype=float32)]\n",
      "Cosine scores: [array([[0.0683279]], dtype=float32), array([[0.14804739]], dtype=float32), array([[0.14860673]], dtype=float32), array([[0.1707281]], dtype=float32)]\n",
      "Cosine scores: [array([[0.18426214]], dtype=float32), array([[0.22673973]], dtype=float32), array([[0.2526833]], dtype=float32), array([[0.2705803]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\prasr\\AppData\\Local\\Temp\\ipykernel_21396\\2651588844.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df[['consine_sim','predicted_answer']] = temp_df[['context','question']]\\\n",
      "C:\\Users\\prasr\\AppData\\Local\\Temp\\ipykernel_21396\\2651588844.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df[['consine_sim','predicted_answer']] = temp_df[['context','question']]\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "      <th>consine_sim</th>\n",
       "      <th>predicted_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.22614476]]</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.21846901]]</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>False</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "      <td>[[0.50080836]]</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.1707281]]</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.2705803]]</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "2  56be85543aeaaa14008c9066  Beyoncé   \n",
       "3  56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4  56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "   answer_start  is_impossible  \\\n",
       "0           269          False   \n",
       "1           207          False   \n",
       "2           526          False   \n",
       "3           166          False   \n",
       "4           276          False   \n",
       "\n",
       "                                    answer_sentences     consine_sim  \\\n",
       "0  Born and raised in Houston, Texas, she perform...  [[0.22614476]]   \n",
       "1  Born and raised in Houston, Texas, she perform...  [[0.21846901]]   \n",
       "2  Their hiatus saw the release of Beyoncé's debu...  [[0.50080836]]   \n",
       "3  Born and raised in Houston, Texas, she perform...   [[0.1707281]]   \n",
       "4  Born and raised in Houston, Texas, she perform...   [[0.2705803]]   \n",
       "\n",
       "                                    predicted_answer  \n",
       "0  Their hiatus saw the release of Beyoncé's debu...  \n",
       "1  Born and raised in Houston, Texas, she perform...  \n",
       "2  Born and raised in Houston, Texas, she perform...  \n",
       "3  Their hiatus saw the release of Beyoncé's debu...  \n",
       "4  Their hiatus saw the release of Beyoncé's debu...  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[['consine_sim','predicted_answer']] = temp_df[['context','question']]\\\n",
    ".progress_apply(lambda x: get_cosine_similarity(x[0],x[1],w2v_model),axis=1,result_type=\"expand\")\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1081b",
   "metadata": {},
   "source": [
    "### Evaluvate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd92b12",
   "metadata": {},
   "source": [
    "#### On Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "57e97742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 86820/86820 [03:52<00:00, 374.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "      <th>consine_sim</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>correct_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.22614476]]</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.21846901]]</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>False</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "      <td>[[0.50080836]]</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.1707281]]</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.2705803]]</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86815</th>\n",
       "      <td>5735d259012e2f140011a09d</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>229</td>\n",
       "      <td>False</td>\n",
       "      <td>KMC's first international relationship was est...</td>\n",
       "      <td>[[0.4740299]]</td>\n",
       "      <td>KMC's first international relationship was est...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86816</th>\n",
       "      <td>5735d259012e2f140011a09e</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What was Yangon previously known as?</td>\n",
       "      <td>Rangoon</td>\n",
       "      <td>414</td>\n",
       "      <td>False</td>\n",
       "      <td>This activity has been further enhanced by est...</td>\n",
       "      <td>[[0.34433782]]</td>\n",
       "      <td>KMC's first international relationship was est...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86817</th>\n",
       "      <td>5735d259012e2f140011a09f</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>476</td>\n",
       "      <td>False</td>\n",
       "      <td>This activity has been further enhanced by est...</td>\n",
       "      <td>[[0.3677879]]</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86818</th>\n",
       "      <td>5735d259012e2f140011a0a0</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>1975</td>\n",
       "      <td>199</td>\n",
       "      <td>False</td>\n",
       "      <td>KMC's first international relationship was est...</td>\n",
       "      <td>[[0.41204268]]</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86819</th>\n",
       "      <td>5735d259012e2f140011a0a1</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>[[0.468301]]</td>\n",
       "      <td>This activity has been further enhanced by est...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86820 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id      title  \\\n",
       "0      56be85543aeaaa14008c9063    Beyoncé   \n",
       "1      56be85543aeaaa14008c9065    Beyoncé   \n",
       "2      56be85543aeaaa14008c9066    Beyoncé   \n",
       "3      56bf6b0f3aeaaa14008c9601    Beyoncé   \n",
       "4      56bf6b0f3aeaaa14008c9602    Beyoncé   \n",
       "...                         ...        ...   \n",
       "86815  5735d259012e2f140011a09d  Kathmandu   \n",
       "86816  5735d259012e2f140011a09e  Kathmandu   \n",
       "86817  5735d259012e2f140011a09f  Kathmandu   \n",
       "86818  5735d259012e2f140011a0a0  Kathmandu   \n",
       "86819  5735d259012e2f140011a0a1  Kathmandu   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "...                                                  ...   \n",
       "86815  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "86816  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "86817  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "86818  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "86819  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                                question  \\\n",
       "0               When did Beyonce start becoming popular?   \n",
       "1      What areas did Beyonce compete in when she was...   \n",
       "2      When did Beyonce leave Destiny's Child and bec...   \n",
       "3          In what city and state did Beyonce  grow up?    \n",
       "4             In which decade did Beyonce become famous?   \n",
       "...                                                  ...   \n",
       "86815  In what US state did Kathmandu first establish...   \n",
       "86816               What was Yangon previously known as?   \n",
       "86817  With what Belorussian city does Kathmandu have...   \n",
       "86818  In what year did Kathmandu create its initial ...   \n",
       "86819                      What is KMC an initialism of?   \n",
       "\n",
       "                            answer  answer_start  is_impossible  \\\n",
       "0                in the late 1990s           269          False   \n",
       "1              singing and dancing           207          False   \n",
       "2                             2003           526          False   \n",
       "3                   Houston, Texas           166          False   \n",
       "4                       late 1990s           276          False   \n",
       "...                            ...           ...            ...   \n",
       "86815                       Oregon           229          False   \n",
       "86816                      Rangoon           414          False   \n",
       "86817                        Minsk           476          False   \n",
       "86818                         1975           199          False   \n",
       "86819  Kathmandu Metropolitan City             0          False   \n",
       "\n",
       "                                        answer_sentences     consine_sim  \\\n",
       "0      Born and raised in Houston, Texas, she perform...  [[0.22614476]]   \n",
       "1      Born and raised in Houston, Texas, she perform...  [[0.21846901]]   \n",
       "2      Their hiatus saw the release of Beyoncé's debu...  [[0.50080836]]   \n",
       "3      Born and raised in Houston, Texas, she perform...   [[0.1707281]]   \n",
       "4      Born and raised in Houston, Texas, she perform...   [[0.2705803]]   \n",
       "...                                                  ...             ...   \n",
       "86815  KMC's first international relationship was est...   [[0.4740299]]   \n",
       "86816  This activity has been further enhanced by est...  [[0.34433782]]   \n",
       "86817  This activity has been further enhanced by est...   [[0.3677879]]   \n",
       "86818  KMC's first international relationship was est...  [[0.41204268]]   \n",
       "86819  Kathmandu Metropolitan City (KMC), in order to...    [[0.468301]]   \n",
       "\n",
       "                                        predicted_answer  correct_prediction  \n",
       "0      Their hiatus saw the release of Beyoncé's debu...               False  \n",
       "1      Born and raised in Houston, Texas, she perform...                True  \n",
       "2      Born and raised in Houston, Texas, she perform...               False  \n",
       "3      Their hiatus saw the release of Beyoncé's debu...               False  \n",
       "4      Their hiatus saw the release of Beyoncé's debu...               False  \n",
       "...                                                  ...                 ...  \n",
       "86815  KMC's first international relationship was est...                True  \n",
       "86816  KMC's first international relationship was est...               False  \n",
       "86817  Kathmandu Metropolitan City (KMC), in order to...               False  \n",
       "86818  Kathmandu Metropolitan City (KMC), in order to...               False  \n",
       "86819  This activity has been further enhanced by est...               False  \n",
       "\n",
       "[86820 rows x 11 columns]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['consine_sim','predicted_answer']] = train_df[['context','question']]\\\n",
    ".progress_apply(lambda x: get_cosine_similarity(x[0],x[1],w2v_model),axis=1,result_type=\"expand\")\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "6959216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     61856\n",
       "False    24964\n",
       "Name: correct_prediction, dtype: int64"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['correct_prediction'] = train_df['answer_sentences'] == train_df['predicted_answer']\n",
    "train_df['correct_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "6c5668ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7124625662289795\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {train_df[train_df['correct_prediction']].shape[0]/train_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac35216",
   "metadata": {},
   "source": [
    "#### On Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "032a1a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20302/20302 [00:56<00:00, 359.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "      <th>consine_sim</th>\n",
       "      <th>predicted_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>[[0.2944552]]</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>[[0.2944552]]</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56ddde6b9a695914005b9628  Normans   \n",
       "1  56ddde6b9a695914005b9628  Normans   \n",
       "\n",
       "                                             context  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                               question  answer  answer_start  is_impossible  \\\n",
       "0  In what country is Normandy located?  France           159          False   \n",
       "1  In what country is Normandy located?  France           159          False   \n",
       "\n",
       "                                    answer_sentences    consine_sim  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...  [[0.2944552]]   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...  [[0.2944552]]   \n",
       "\n",
       "                                    predicted_answer  \n",
       "0  The Normans (Norman: Nourmands; French: Norman...  \n",
       "1  The Normans (Norman: Nourmands; French: Norman...  "
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[['consine_sim','predicted_answer']] = val_df[['context','question']]\\\n",
    ".progress_apply(lambda x: get_cosine_similarity(x[0],x[1],w2v_model),axis=1,result_type=\"expand\")\n",
    "val_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "c2824fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     14714\n",
       "False     5588\n",
       "Name: correct_prediction, dtype: int64"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['correct_prediction'] = val_df['answer_sentences'] == val_df['predicted_answer']\n",
    "val_df['correct_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "5903372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7247561816569796\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {val_df[val_df['correct_prediction']].shape[0]/val_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f219ab9",
   "metadata": {},
   "source": [
    "### Download word2vec model google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96913367",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "2277c43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "43061135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Q: When did Beyonce start becoming popular?\n",
      "[['<UNK>', '<UNK>', '<UNK>', 'carter', '<UNK>', 'bee', 'yon', 'say', 'born', 'september', 'is', 'an', 'american', 'singer', 'songwriter', 'record', 'producer', '<UNK>', 'actress'], ['born', '<UNK>', 'raised', 'in', 'houston', 'texas', 'she', 'performed', 'in', 'various', 'singing', '<UNK>', 'dancing', 'competitions', 'as', 'child', '<UNK>', 'rose', '<UNK>', 'fame', 'in', 'the', 'late', 'as', 'lead', 'singer', '<UNK>', 'girl', 'group', 'destiny', 'child'], ['managed', 'by', 'her', 'father', '<UNK>', '<UNK>', 'the', 'group', 'became', 'one', '<UNK>', 'the', 'world', 'best', 'selling', 'girl', 'groups', '<UNK>', 'all', 'time'], ['their', 'hiatus', 'saw', 'the', 'release', '<UNK>', '<UNK>', 'debut', 'album', 'dangerously', 'in', 'love', 'which', 'established', 'her', 'as', 'solo', 'artist', 'worldwide', 'earned', 'five', 'grammy', 'awards', '<UNK>', 'featured', 'the', 'billboard', 'hot', 'number', 'one', 'singles', 'crazy', 'in', 'love', '<UNK>', 'baby', 'boy']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.6053659]], dtype=float32),\n",
       " \"Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\")"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_context = train_df['context'].tolist()[0]\n",
    "sample_question = train_df['question'].tolist()[0]\n",
    "print(f\"C:{sample_context}\")\n",
    "print(f\"Q: {sample_question}\")\n",
    "get_cosine_similarity(sample_context,sample_question,google_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "7e4252bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['world'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b893aea",
   "metadata": {},
   "source": [
    "### Evaluvate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "aac9592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model, num_features):\n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        word_vec_model = model.wv\n",
    "    else:\n",
    "        word_vec_model = model\n",
    "    index2word_set = word_vec_model.index_to_key \n",
    "    #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, word_vec_model[word])\n",
    "\n",
    "    if nwords>0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "#     print(featureVec)\n",
    "    return featureVec\n",
    "def get_context_vector(context,model,vector_size=300):\n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        vocab = model.wv.key_to_index\n",
    "    else:\n",
    "        vocab = model.key_to_index\n",
    "    context_sents = sent_tokenize(context)\n",
    "    processed_context = [simple_preprocess(sent) for sent in context_sents]\n",
    "    processed_context = [[word if word in vocab else UNK for word in processed_context_sent]\\\n",
    "                         for processed_context_sent in processed_context]\n",
    "    context_vectors = [np.array(avg_sentence_vector(processed_context_sent,model,vector_size)).reshape(1,-1) for processed_context_sent in processed_context]\n",
    "    \n",
    "    return context_vectors\n",
    "    \n",
    "def get_cosine_similarity(context,context_vectors,question,model,vector_size=300):\n",
    "    context_sents = sent_tokenize(context)\n",
    "    \n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        vocab = model.wv.key_to_index\n",
    "    else:\n",
    "        vocab = model.key_to_index\n",
    "        \n",
    "    processed_question = simple_preprocess(question)\n",
    "    processed_question = [word if word in vocab else UNK for word in processed_question]\n",
    "    \n",
    "    question_vector  = np.array(avg_sentence_vector(processed_question,model,vector_size)).reshape(1,-1)\n",
    "    \n",
    "    cosine_sim_list = [cosine_similarity(context_sent_vector,question_vector) for context_sent_vector in context_vectors]\n",
    "    \n",
    "    max_cosine_sim = max(cosine_sim_list)\n",
    "    predicted_answer = context_sents[np.argmax(cosine_sim_list)]\n",
    "    return max_cosine_sim, predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "70b6a3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb003432baee4ddba30d6c8cf9832d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prasr\\AppData\\Local\\Temp\\ipykernel_21396\\1690198111.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['context_vec'] = temp_df['context'].swifter\\\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f897be51b643bdbb2f6f6279cd1e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prasr\\AppData\\Local\\Temp\\ipykernel_21396\\1690198111.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df[['consine_sim','predicted_answer']] = temp_df[['context','context_vec','question']]\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "      <th>consine_sim</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>context_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.6053659]]</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>[[[-0.011311122, -0.024881635, -0.053231377, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>[[0.6998993]]</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>[[[-0.011311122, -0.024881635, -0.053231377, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "\n",
       "   answer_start  is_impossible  \\\n",
       "0           269          False   \n",
       "1           207          False   \n",
       "\n",
       "                                    answer_sentences    consine_sim  \\\n",
       "0  Born and raised in Houston, Texas, she perform...  [[0.6053659]]   \n",
       "1  Born and raised in Houston, Texas, she perform...  [[0.6998993]]   \n",
       "\n",
       "                                    predicted_answer  \\\n",
       "0  Managed by her father, Mathew Knowles, the gro...   \n",
       "1  Managed by her father, Mathew Knowles, the gro...   \n",
       "\n",
       "                                         context_vec  \n",
       "0  [[[-0.011311122, -0.024881635, -0.053231377, 0...  \n",
       "1  [[[-0.011311122, -0.024881635, -0.053231377, 0...  "
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['context_vec'] = temp_df['context'].swifter\\\n",
    ".progress_bar(enable=True, desc=None).apply(lambda x: get_context_vector(x,google_model))\n",
    "\n",
    "temp_df[['consine_sim','predicted_answer']] = temp_df[['context','context_vec','question']]\\\n",
    ".swifter.progress_bar(enable=True, desc=None)\\\n",
    ".apply(lambda x: get_cosine_similarity(x[0],x[1],x[2],google_model,300),axis=1,result_type=\"expand\")\n",
    "temp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "8d6160d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df[['consine_sim','predicted_answer']] = temp_df[['context','question']]\\\n",
    "# .swifter.progress_bar(enable=True, desc=None)\\\n",
    "# .apply(lambda x: get_cosine_similarity(x[0],x[1],google_model,300),axis=1,result_type=\"expand\")\n",
    "# temp_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4d512",
   "metadata": {},
   "source": [
    "#### On val set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "e038f510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa61e5eaa7541778cd3c0171e6cefe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_df['context_vec'] = val_df['context'].swifter\\\n",
    ".progress_bar(enable=True, desc=None).apply(lambda x: get_context_vector(x,google_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "b7e86cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054f521b48f9466d8479c65257586078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "      <th>consine_sim</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>correct_prediction</th>\n",
       "      <th>context_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>[[0.50003517]]</td>\n",
       "      <td>The distinct cultural and ethnic identity of t...</td>\n",
       "      <td>True</td>\n",
       "      <td>[[[0.064170435, 0.075368784, 0.09860872, 0.118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>[[0.50003517]]</td>\n",
       "      <td>The distinct cultural and ethnic identity of t...</td>\n",
       "      <td>True</td>\n",
       "      <td>[[[0.064170435, 0.075368784, 0.09860872, 0.118...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56ddde6b9a695914005b9628  Normans   \n",
       "1  56ddde6b9a695914005b9628  Normans   \n",
       "\n",
       "                                             context  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                               question  answer  answer_start  is_impossible  \\\n",
       "0  In what country is Normandy located?  France           159          False   \n",
       "1  In what country is Normandy located?  France           159          False   \n",
       "\n",
       "                                    answer_sentences     consine_sim  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...  [[0.50003517]]   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...  [[0.50003517]]   \n",
       "\n",
       "                                    predicted_answer  correct_prediction  \\\n",
       "0  The distinct cultural and ethnic identity of t...                True   \n",
       "1  The distinct cultural and ethnic identity of t...                True   \n",
       "\n",
       "                                         context_vec  \n",
       "0  [[[0.064170435, 0.075368784, 0.09860872, 0.118...  \n",
       "1  [[[0.064170435, 0.075368784, 0.09860872, 0.118...  "
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[['consine_sim','predicted_answer']] = val_df[['context','context_vec','question']]\\\n",
    ".swifter.progress_bar(enable=True, desc=None)\\\n",
    ".apply(lambda x: get_cosine_similarity(x[0],x[1],x[2],google_model,300),axis=1,result_type=\"expand\")\n",
    "val_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "e4f0b77d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     14660\n",
       "False     5642\n",
       "Name: correct_prediction, dtype: int64"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['correct_prediction'] = val_df['answer_sentences'] == val_df['predicted_answer']\n",
    "val_df['correct_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "dde95c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7220963451876662\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {val_df[val_df['correct_prediction']].shape[0]/val_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c12acf",
   "metadata": {},
   "source": [
    "### Training a fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a817287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 78414\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "vector_size = 400\n",
    "fast_text_model = FastText(min_count=1, \n",
    "                     window = 5,\n",
    "                     vector_size = vector_size,\n",
    "                     sg = 1,\n",
    "                     hs = 1,\n",
    "                     workers=10)\n",
    "fast_text_model.build_vocab(train_sentences)\n",
    "words = fast_text_model.wv.key_to_index.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)\n",
    "# Train Word Embeddings\n",
    "fast_text_model.train(train_sentences, \n",
    "                total_examples=fast_text_model.corpus_count, \n",
    "                epochs=500, \n",
    "                report_delay=1,\n",
    "                compute_loss = True,) # set compute_loss = True\n",
    "#                 callbacks=[callback()]) \n",
    "print(fast_text_model.get_latest_training_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb67a5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('period', 0.46009957790374756),\n",
       " ('gamedaily', 0.3613353967666626),\n",
       " ('during', 0.34807538986206055),\n",
       " ('beatle', 0.3249432444572449),\n",
       " ('same', 0.3225545287132263),\n",
       " ('periods', 0.32006415724754333),\n",
       " ('johnathon', 0.319020539522171),\n",
       " ('span', 0.31888797879219055),\n",
       " ('ascession', 0.31213393807411194),\n",
       " ('accension', 0.30582237243652344)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text_model.wv.most_similar(positive=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3b29480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4006621\n"
     ]
    }
   ],
   "source": [
    "print(fast_text_model.wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfc1dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(context,question,model,vector_size=300):\n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        vocab = model.wv.key_to_index\n",
    "    else:\n",
    "        vocab = model.key_to_index\n",
    "    context_sents = sent_tokenize(context)\n",
    "    \n",
    "    processed_context = [simple_preprocess(sent) for sent in context_sents]\n",
    "#     processed_context = [[word if word in vocab else UNK for word in processed_context_sent]\\\n",
    "#                          for processed_context_sent in processed_context]\n",
    "    processed_question = simple_preprocess(question)\n",
    "#     print(processed_context)\n",
    "#     processed_question = [word if word in vocab else UNK for word in processed_question]\n",
    "    \n",
    "#     context_vectors = [np.array(avg_sentence_vector(processed_context_sent,model,vector_size)).reshape(1,-1) for processed_context_sent in processed_context]\n",
    "#     question_vector  = np.array(avg_sentence_vector(processed_question,model,vector_size)).reshape(1,-1)\n",
    "#     print(len(context_vectors[0]))\n",
    "#     print(cosine_similarity(np.array(context_vectors[0]).reshape(1,-1),np.array(question_vector).reshape(1,-1)))\n",
    "    \n",
    "#     cosine_sim_list = [cosine_similarity(context_sent_vector,question_vector) for context_sent_vector in context_vectors]\n",
    "        #     print(f\"Cosine scores: {cosine_sim_list}\")\n",
    "    cosine_sim_list = [model.wv.n_similarity(context_sent,processed_question) for context_sent in processed_context if len(context_sent) > 0]\n",
    "#     print(cosine_sim_list)\n",
    "    max_cosine_sim = max(cosine_sim_list)\n",
    "    predicted_answer = context_sents[np.argmax(cosine_sim_list)]\n",
    "    return max_cosine_sim, predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a41af1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Q: What areas did Beyonce compete in when she was growing up?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5806109,\n",
       " \"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_context = train_df['context'].tolist()[0]\n",
    "sample_question = train_df['question'].tolist()[1]\n",
    "print(f\"C:{sample_context}\")\n",
    "print(f\"Q: {sample_question}\")\n",
    "get_cosine_similarity(sample_context,sample_question,fast_text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2359fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 86820/86820 [01:01<00:00, 1422.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "      <th>consine_sim</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>correct_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>0.508958</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>0.580611</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "\n",
       "   answer_start  is_impossible  \\\n",
       "0           269          False   \n",
       "1           207          False   \n",
       "\n",
       "                                    answer_sentences  consine_sim  \\\n",
       "0  Born and raised in Houston, Texas, she perform...     0.508958   \n",
       "1  Born and raised in Houston, Texas, she perform...     0.580611   \n",
       "\n",
       "                                    predicted_answer  correct_prediction  \n",
       "0  Born and raised in Houston, Texas, she perform...                True  \n",
       "1  Born and raised in Houston, Texas, she perform...                True  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['consine_sim','predicted_answer']] = train_df[['context','question']]\\\n",
    ".progress_apply(lambda x: get_cosine_similarity(x[0],x[1],fast_text_model,100),axis=1,result_type=\"expand\")\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6804005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     61594\n",
       "False    25226\n",
       "Name: correct_prediction, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['correct_prediction'] = train_df['answer_sentences'] == train_df['predicted_answer']\n",
    "train_df['correct_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a83be784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7094448283805574\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {train_df[train_df['correct_prediction']].shape[0]/train_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74aedd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 20302/20302 [00:22<00:00, 919.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_sentences</th>\n",
       "      <th>consine_sim</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>correct_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>0.621735</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>0.621735</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56ddde6b9a695914005b9628  Normans   \n",
       "1  56ddde6b9a695914005b9628  Normans   \n",
       "\n",
       "                                             context  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                               question  answer  answer_start  is_impossible  \\\n",
       "0  In what country is Normandy located?  France           159          False   \n",
       "1  In what country is Normandy located?  France           159          False   \n",
       "\n",
       "                                    answer_sentences  consine_sim  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...     0.621735   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...     0.621735   \n",
       "\n",
       "                                    predicted_answer  correct_prediction  \n",
       "0  The Normans (Norman: Nourmands; French: Norman...                True  \n",
       "1  The Normans (Norman: Nourmands; French: Norman...                True  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[['consine_sim','predicted_answer']] = val_df[['context','question']]\\\n",
    ".progress_apply(lambda x: get_cosine_similarity(x[0],x[1],fast_text_model),axis=1,result_type=\"expand\")\n",
    "val_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a02f22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     14381\n",
       "False     5921\n",
       "Name: correct_prediction, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['correct_prediction'] = val_df['answer_sentences'] == val_df['predicted_answer']\n",
    "val_df['correct_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba0bb972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7083538567628805\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {val_df[val_df['correct_prediction']].shape[0]/val_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a702053",
   "metadata": {},
   "source": [
    "#### Download and use GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42e7ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f70c22d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 15.6% 58.6/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============------------------------------------] 28.9% 108.5/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================--------------------------------] 36.6% 137.6/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================-------------------------] 50.8% 190.9/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================-----------] 79.2% 297.8/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================------] 88.7% 333.6/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 97.0% 365.0/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glove_model = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0824108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model, num_features):\n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        word_vec_model = model.wv\n",
    "    else:\n",
    "        word_vec_model = model\n",
    "    index2word_set = word_vec_model.index_to_key \n",
    "    #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, word_vec_model[word])\n",
    "\n",
    "    if nwords>0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "#     print(featureVec)\n",
    "    return featureVec\n",
    "def get_context_vector(context,model,vector_size=300):\n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        vocab = model.wv.key_to_index\n",
    "    else:\n",
    "        vocab = model.key_to_index\n",
    "    context_sents = sent_tokenize(context)\n",
    "    processed_context = [simple_preprocess(sent) for sent in context_sents]\n",
    "    processed_context = [[word if word in vocab else UNK for word in processed_context_sent]\\\n",
    "                         for processed_context_sent in processed_context]\n",
    "    context_vectors = [np.array(avg_sentence_vector(processed_context_sent,model,vector_size)).reshape(1,-1) for processed_context_sent in processed_context]\n",
    "    \n",
    "    return context_vectors\n",
    "    \n",
    "def get_cosine_similarity(context,context_vectors,question,model,vector_size=300):\n",
    "    context_sents = sent_tokenize(context)\n",
    "    \n",
    "    if isinstance(model,gensim.models.word2vec.Word2Vec):\n",
    "        vocab = model.wv.key_to_index\n",
    "    else:\n",
    "        vocab = model.key_to_index\n",
    "        \n",
    "    processed_question = simple_preprocess(question)\n",
    "    processed_question = [word if word in vocab else UNK for word in processed_question]\n",
    "    \n",
    "    question_vector  = np.array(avg_sentence_vector(processed_question,model,vector_size)).reshape(1,-1)\n",
    "    \n",
    "    cosine_sim_list = [cosine_similarity(context_sent_vector,question_vector) for context_sent_vector in context_vectors]\n",
    "    \n",
    "    max_cosine_sim = max(cosine_sim_list)\n",
    "    predicted_answer = context_sents[np.argmax(cosine_sim_list)]\n",
    "    return max_cosine_sim, predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "760a4a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5d49ede2944aa6be2da769f01cd446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/20302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_df['context_vec'] = val_df['context'].swifter\\\n",
    ".progress_bar(enable=True, desc=None).apply(lambda x: get_context_vector(x,glove_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4698f852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     14381\n",
       "False     5921\n",
       "Name: correct_prediction, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['correct_prediction'] = val_df['answer_sentences'] == val_df['predicted_answer']\n",
    "val_df['correct_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9648760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7083538567628805\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {val_df[val_df['correct_prediction']].shape[0]/val_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bad28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76670389a092d5ff6315c39b8f6840652919966ad836ef8d70af66b50d13fe8a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
